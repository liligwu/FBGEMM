// !!! This is a file automatically generated by hipify!!!
#include "hip/hip_runtime.h"
/*
 * Copyright (c) Meta Platforms, Inc. and affiliates.
 * All rights reserved.
 *
 * This source code is licensed under the BSD-style license found in the
 * LICENSE file in the root directory of this source tree.
 */


#include "fbgemm_gpu/utils/dispatch_macros_hip.h"
////////////////////////////////////////////////////////////////////////////////
// Required for op registrations
////////////////////////////////////////////////////////////////////////////////
#include "fbgemm_gpu/utils/ops_utils.h"
#include "fbgemm_gpu/embedding_forward_template_helpers_hip.cuh"
#include "fbgemm_gpu/split_embeddings_cache_cuda.cuh"
#include <torch/torch.h>
#include "gen_embedding_forward_split_unweighted_v2_kernel.hip"


using Tensor = at::Tensor;
using namespace fbgemm_gpu;

////////////////////////////////////////////////////////////////////////////////
// External Function Declarations
////////////////////////////////////////////////////////////////////////////////

// #ifndef USE_ROCM
// Support only the split-pooled TBE case
template <
    typename emb_t,
    typename cache_t,
    typename output_t,
    typename index_t,
    bool use_lxu_cache
    >
__launch_bounds__(kForwardMaxThreads, 2048 / kForwardMaxThreads)
__global__ void split_embedding_codegen_forward_unweighted_v2_kernel(
    const emb_t* __restrict__ const dev_weights,
    const emb_t* __restrict__ const uvm_weights,
    const cache_t* __restrict__ const lxu_cache_weights,
    const int32_t* __restrict__ const weights_placements,
    const uint32_t B,
    const uint32_t T,
    const bool mean_pooling,
    const uint32_t max_D_cache,
    const FixedDivisor fd_num_warps_per_table,
    const index_t* __restrict__ const indices,
    const index_t* __restrict__ const  offsets,
    const uint32_t* __restrict__ const D_offsets,
    const int64_t* __restrict__ const weights_offsets,
    const int32_t* __restrict__ const lxu_cache_locations,
    output_t* __restrict__ const output);
// #endif


template <
    typename emb_t,
    typename cache_t,
    typename output_t,
    bool use_lxu_cache,
    typename index_t,
    size_t kMaxVecsPerThread,
    size_t kThreadGroupSize = kWarpSize
    >
__launch_bounds__(kForwardMaxThreads) __global__ void
split_embedding_codegen_forward_unweighted_kernel(
    const pta::PackedTensorAccessor64<emb_t, 1, at::RestrictPtrTraits> dev_weights,
    const pta::PackedTensorAccessor64<emb_t, 1, at::RestrictPtrTraits> uvm_weights,
    const pta::PackedTensorAccessor64<cache_t, 2, at::RestrictPtrTraits> lxu_cache_weights,
    const pta::PackedTensorAccessor32<int32_t, 1, at::RestrictPtrTraits> weights_placements,
    const pta::PackedTensorAccessor32<int64_t, 1, at::RestrictPtrTraits> weights_offsets,
    const pta::PackedTensorAccessor32<int32_t, 1, at::RestrictPtrTraits> D_offsets,
    FixedDivisor fd_B,
    const pta::PackedTensorAccessor32<index_t, 1, at::RestrictPtrTraits> indices,
    const pta::PackedTensorAccessor32<index_t, 1, at::RestrictPtrTraits> offsets,
    int64_t pooling_mode,
    const pta::PackedTensorAccessor32<int32_t, 1, at::RestrictPtrTraits> lxu_cache_locations,
    const int32_t* lxu_cache_conflict_misses, // if dense
    pta::PackedTensorAccessor64<output_t, 2, at::RestrictPtrTraits> output
    );


////////////////////////////////////////////////////////////////////////////////
// Utility Macros
////////////////////////////////////////////////////////////////////////////////

/*
  The macro definition for both cases are almost the same except for the
  definition of kThreadGroupSize.  In the FBGEMM_USE_SUBWARP_SHUFFLE case, if
  MAX_D is small, then we use fewer number of threads than kWarpSize.

  NOTE: kMaxVecsPerThread is computed using the ternary operator because HIPCC
  is unable to use std::max in constexpr context.
*/

#ifdef FBGEMM_USE_SUBWARP_SHUFFLE
#define DISPATCH_OPTIMAL_FORWARD_KERNEL(MAX_D, ...) \
  [&] {                                        \
    if (MAX_D <= 32) {                               \
             [[ maybe_unused ]] const int max_vecs_per_thread =               \
               1;                                      \
             constexpr int kFixedMaxVecsPerThread = 1; \
             [[ maybe_unused ]] constexpr int kThreadGroupSize =              \
               8;                                            \
             [[ maybe_unused ]] constexpr bool kUseVecBlocking =              \
               false;                                             \
             return __VA_ARGS__();                                            \
           }                                                                  \
        if (MAX_D <= 64) {                               \
             [[ maybe_unused ]] const int max_vecs_per_thread =               \
               1;                                      \
             constexpr int kFixedMaxVecsPerThread = 1; \
             [[ maybe_unused ]] constexpr int kThreadGroupSize =              \
               16;                                            \
             [[ maybe_unused ]] constexpr bool kUseVecBlocking =              \
               false;                                             \
             return __VA_ARGS__();                                            \
           }                                                                  \
        if (MAX_D <= 128) {                               \
             [[ maybe_unused ]] const int max_vecs_per_thread =               \
               1;                                      \
             constexpr int kFixedMaxVecsPerThread = 1; \
             [[ maybe_unused ]] constexpr int kThreadGroupSize =              \
               32;                                            \
             [[ maybe_unused ]] constexpr bool kUseVecBlocking =              \
               false;                                             \
             return __VA_ARGS__();                                            \
           }                                                                  \
        if (MAX_D <= 256) {                               \
             [[ maybe_unused ]] const int max_vecs_per_thread =               \
               1;                                      \
             constexpr int kFixedMaxVecsPerThread = 1; \
             [[ maybe_unused ]] constexpr int kThreadGroupSize =              \
               64;                                            \
             [[ maybe_unused ]] constexpr bool kUseVecBlocking =              \
               false;                                             \
             return __VA_ARGS__();                                            \
           }                                                                  \
        if (MAX_D <= 512) {                               \
             [[ maybe_unused ]] const int max_vecs_per_thread =               \
               2;                                      \
             constexpr int kFixedMaxVecsPerThread = 2; \
             [[ maybe_unused ]] constexpr int kThreadGroupSize =              \
               64;                                            \
             [[ maybe_unused ]] constexpr bool kUseVecBlocking =              \
               false;                                             \
             return __VA_ARGS__();                                            \
           }                                                                  \
        if (MAX_D <= 768) {                               \
             [[ maybe_unused ]] const int max_vecs_per_thread =               \
               3;                                      \
             constexpr int kFixedMaxVecsPerThread = 3; \
             [[ maybe_unused ]] constexpr int kThreadGroupSize =              \
               64;                                            \
             [[ maybe_unused ]] constexpr bool kUseVecBlocking =              \
               false;                                             \
             return __VA_ARGS__();                                            \
           }                                                                  \
        if (MAX_D <= 1024) {                               \
             [[ maybe_unused ]] const int max_vecs_per_thread =               \
               4;                                      \
             constexpr int kFixedMaxVecsPerThread = 4; \
             [[ maybe_unused ]] constexpr int kThreadGroupSize =              \
               64;                                            \
             [[ maybe_unused ]] constexpr bool kUseVecBlocking =              \
               false;                                             \
             return __VA_ARGS__();                                            \
           }                                                                  \
        if (MAX_D <= 1280) {                               \
             [[ maybe_unused ]] const int max_vecs_per_thread =               \
               5;                                      \
             constexpr int kFixedMaxVecsPerThread = 5; \
             [[ maybe_unused ]] constexpr int kThreadGroupSize =              \
               64;                                            \
             [[ maybe_unused ]] constexpr bool kUseVecBlocking =              \
               false;                                             \
             return __VA_ARGS__();                                            \
           }                                                                  \
        if (MAX_D <= 1536) {                               \
             [[ maybe_unused ]] const int max_vecs_per_thread =               \
               6;                                      \
             constexpr int kFixedMaxVecsPerThread = 6; \
             [[ maybe_unused ]] constexpr int kThreadGroupSize =              \
               64;                                            \
             [[ maybe_unused ]] constexpr bool kUseVecBlocking =              \
               false;                                             \
             return __VA_ARGS__();                                            \
           }                                                                  \
        if (MAX_D <= 1792) {                               \
             [[ maybe_unused ]] const int max_vecs_per_thread =               \
               7;                                      \
             constexpr int kFixedMaxVecsPerThread = 7; \
             [[ maybe_unused ]] constexpr int kThreadGroupSize =              \
               64;                                            \
             [[ maybe_unused ]] constexpr bool kUseVecBlocking =              \
               false;                                             \
             return __VA_ARGS__();                                            \
           }                                                                  \
        if (MAX_D <= 2048) {                               \
             [[ maybe_unused ]] const int max_vecs_per_thread =               \
               8;                                      \
             constexpr int kFixedMaxVecsPerThread = 8; \
             [[ maybe_unused ]] constexpr int kThreadGroupSize =              \
               64;                                            \
             [[ maybe_unused ]] constexpr bool kUseVecBlocking =              \
               false;                                             \
             return __VA_ARGS__();                                            \
           }                                                                  \
        return;                                    \
  }()

#else
#define DISPATCH_OPTIMAL_FORWARD_KERNEL(MAX_D, ...) \
  [&] {                                        \
    if (MAX_D <= 256) {                               \
             [[ maybe_unused ]] const int max_vecs_per_thread =               \
               1;                                      \
             constexpr int kFixedMaxVecsPerThread = 1; \
             [[ maybe_unused ]] constexpr int kThreadGroupSize =              \
               64;                                            \
             [[ maybe_unused ]] constexpr bool kUseVecBlocking =              \
               false;                                             \
             return __VA_ARGS__();                                            \
           }                                                                  \
        if (MAX_D <= 512) {                               \
             [[ maybe_unused ]] const int max_vecs_per_thread =               \
               2;                                      \
             constexpr int kFixedMaxVecsPerThread = 2; \
             [[ maybe_unused ]] constexpr int kThreadGroupSize =              \
               64;                                            \
             [[ maybe_unused ]] constexpr bool kUseVecBlocking =              \
               false;                                             \
             return __VA_ARGS__();                                            \
           }                                                                  \
        if (MAX_D <= 768) {                               \
             [[ maybe_unused ]] const int max_vecs_per_thread =               \
               3;                                      \
             constexpr int kFixedMaxVecsPerThread = 3; \
             [[ maybe_unused ]] constexpr int kThreadGroupSize =              \
               64;                                            \
             [[ maybe_unused ]] constexpr bool kUseVecBlocking =              \
               false;                                             \
             return __VA_ARGS__();                                            \
           }                                                                  \
        if (MAX_D <= 1024) {                               \
             [[ maybe_unused ]] const int max_vecs_per_thread =               \
               4;                                      \
             constexpr int kFixedMaxVecsPerThread = 4; \
             [[ maybe_unused ]] constexpr int kThreadGroupSize =              \
               64;                                            \
             [[ maybe_unused ]] constexpr bool kUseVecBlocking =              \
               false;                                             \
             return __VA_ARGS__();                                            \
           }                                                                  \
        if (MAX_D <= 1280) {                               \
             [[ maybe_unused ]] const int max_vecs_per_thread =               \
               5;                                      \
             constexpr int kFixedMaxVecsPerThread = 5; \
             [[ maybe_unused ]] constexpr int kThreadGroupSize =              \
               64;                                            \
             [[ maybe_unused ]] constexpr bool kUseVecBlocking =              \
               false;                                             \
             return __VA_ARGS__();                                            \
           }                                                                  \
        if (MAX_D <= 1536) {                               \
             [[ maybe_unused ]] const int max_vecs_per_thread =               \
               6;                                      \
             constexpr int kFixedMaxVecsPerThread = 6; \
             [[ maybe_unused ]] constexpr int kThreadGroupSize =              \
               64;                                            \
             [[ maybe_unused ]] constexpr bool kUseVecBlocking =              \
               false;                                             \
             return __VA_ARGS__();                                            \
           }                                                                  \
        if (MAX_D <= 1792) {                               \
             [[ maybe_unused ]] const int max_vecs_per_thread =               \
               7;                                      \
             constexpr int kFixedMaxVecsPerThread = 7; \
             [[ maybe_unused ]] constexpr int kThreadGroupSize =              \
               64;                                            \
             [[ maybe_unused ]] constexpr bool kUseVecBlocking =              \
               false;                                             \
             return __VA_ARGS__();                                            \
           }                                                                  \
        if (MAX_D <= 2048) {                               \
             [[ maybe_unused ]] const int max_vecs_per_thread =               \
               8;                                      \
             constexpr int kFixedMaxVecsPerThread = 8; \
             [[ maybe_unused ]] constexpr int kThreadGroupSize =              \
               64;                                            \
             [[ maybe_unused ]] constexpr bool kUseVecBlocking =              \
               false;                                             \
             return __VA_ARGS__();                                            \
           }                                                                  \
        return;                                    \
  }()

#endif


#ifdef FBGEMM_USE_SUBWARP_SHUFFLE
#define DISPATCH_OPTIMAL_LEGACY_FORWARD_KERNEL(MAX_D, ...) \
  [&] {                                        \
    if (MAX_D <= 32) {                               \
             [[ maybe_unused ]] const int max_vecs_per_thread =               \
               1;                                      \
             constexpr int kFixedMaxVecsPerThread = 1; \
             [[ maybe_unused ]] constexpr int kThreadGroupSize =              \
               8;                                            \
             [[ maybe_unused ]] constexpr bool kUseVecBlocking =              \
               false;                                             \
             return __VA_ARGS__();                                            \
           }                                                                  \
        if (MAX_D <= 64) {                               \
             [[ maybe_unused ]] const int max_vecs_per_thread =               \
               1;                                      \
             constexpr int kFixedMaxVecsPerThread = 1; \
             [[ maybe_unused ]] constexpr int kThreadGroupSize =              \
               16;                                            \
             [[ maybe_unused ]] constexpr bool kUseVecBlocking =              \
               false;                                             \
             return __VA_ARGS__();                                            \
           }                                                                  \
        if (MAX_D <= 128) {                               \
             [[ maybe_unused ]] const int max_vecs_per_thread =               \
               1;                                      \
             constexpr int kFixedMaxVecsPerThread = 1; \
             [[ maybe_unused ]] constexpr int kThreadGroupSize =              \
               32;                                            \
             [[ maybe_unused ]] constexpr bool kUseVecBlocking =              \
               false;                                             \
             return __VA_ARGS__();                                            \
           }                                                                  \
        if (MAX_D <= 256) {                               \
             [[ maybe_unused ]] const int max_vecs_per_thread =               \
               1;                                      \
             constexpr int kFixedMaxVecsPerThread = 1; \
             [[ maybe_unused ]] constexpr int kThreadGroupSize =              \
               64;                                            \
             [[ maybe_unused ]] constexpr bool kUseVecBlocking =              \
               false;                                             \
             return __VA_ARGS__();                                            \
           }                                                                  \
        if (MAX_D <= 512) {                               \
             [[ maybe_unused ]] const int max_vecs_per_thread =               \
               2;                                      \
             constexpr int kFixedMaxVecsPerThread = 2; \
             [[ maybe_unused ]] constexpr int kThreadGroupSize =              \
               64;                                            \
             [[ maybe_unused ]] constexpr bool kUseVecBlocking =              \
               false;                                             \
             return __VA_ARGS__();                                            \
           }                                                                  \
        if (MAX_D <= 768) {                               \
             [[ maybe_unused ]] const int max_vecs_per_thread =               \
               3;                                      \
             constexpr int kFixedMaxVecsPerThread = 3; \
             [[ maybe_unused ]] constexpr int kThreadGroupSize =              \
               64;                                            \
             [[ maybe_unused ]] constexpr bool kUseVecBlocking =              \
               false;                                             \
             return __VA_ARGS__();                                            \
           }                                                                  \
        if (MAX_D <= 1024) {                               \
             [[ maybe_unused ]] const int max_vecs_per_thread =               \
               4;                                      \
             constexpr int kFixedMaxVecsPerThread = 4; \
             [[ maybe_unused ]] constexpr int kThreadGroupSize =              \
               64;                                            \
             [[ maybe_unused ]] constexpr bool kUseVecBlocking =              \
               false;                                             \
             return __VA_ARGS__();                                            \
           }                                                                  \
        return;                                    \
  }()

#else
#define DISPATCH_OPTIMAL_LEGACY_FORWARD_KERNEL(MAX_D, ...) \
  [&] {                                        \
    if (MAX_D <= 256) {                               \
             [[ maybe_unused ]] const int max_vecs_per_thread =               \
               1;                                      \
             constexpr int kFixedMaxVecsPerThread = 1; \
             [[ maybe_unused ]] constexpr int kThreadGroupSize =              \
               64;                                            \
             [[ maybe_unused ]] constexpr bool kUseVecBlocking =              \
               false;                                             \
             return __VA_ARGS__();                                            \
           }                                                                  \
        if (MAX_D <= 512) {                               \
             [[ maybe_unused ]] const int max_vecs_per_thread =               \
               2;                                      \
             constexpr int kFixedMaxVecsPerThread = 2; \
             [[ maybe_unused ]] constexpr int kThreadGroupSize =              \
               64;                                            \
             [[ maybe_unused ]] constexpr bool kUseVecBlocking =              \
               false;                                             \
             return __VA_ARGS__();                                            \
           }                                                                  \
        if (MAX_D <= 768) {                               \
             [[ maybe_unused ]] const int max_vecs_per_thread =               \
               3;                                      \
             constexpr int kFixedMaxVecsPerThread = 3; \
             [[ maybe_unused ]] constexpr int kThreadGroupSize =              \
               64;                                            \
             [[ maybe_unused ]] constexpr bool kUseVecBlocking =              \
               false;                                             \
             return __VA_ARGS__();                                            \
           }                                                                  \
        if (MAX_D <= 1024) {                               \
             [[ maybe_unused ]] const int max_vecs_per_thread =               \
               4;                                      \
             constexpr int kFixedMaxVecsPerThread = 4; \
             [[ maybe_unused ]] constexpr int kThreadGroupSize =              \
               64;                                            \
             [[ maybe_unused ]] constexpr bool kUseVecBlocking =              \
               false;                                             \
             return __VA_ARGS__();                                            \
           }                                                                  \
        return;                                    \
  }()

#endif


#define DISPATCH_OPTIMAL_NOBAG_FORWARD_KERNEL(DD_, ...)                        \
  [&] {                                                                        \
    if (DD_ <= 4) {                                         \
      constexpr int kEmbeddingSize = 4;                     \
      return __VA_ARGS__();                                                    \
    }                                                                          \
    if (DD_ <= 8) {                                         \
      constexpr int kEmbeddingSize = 8;                     \
      return __VA_ARGS__();                                                    \
    }                                                                          \
    if (DD_ <= 16) {                                         \
      constexpr int kEmbeddingSize = 16;                     \
      return __VA_ARGS__();                                                    \
    }                                                                          \
    if (DD_ <= 32) {                                         \
      constexpr int kEmbeddingSize = 32;                     \
      return __VA_ARGS__();                                                    \
    }                                                                          \
    return;                                                                    \
  }()


#define DISPATCH_KERNEL_FOR_CACHE_CASE(CACHE_CASE_, ...)                       \
  [&] {                                                                        \
    if (CACHE_CASE_ == false) {                                      \
      constexpr auto use_cache_t = false;                            \
      return __VA_ARGS__();                                                    \
    }                                                                          \
    if (CACHE_CASE_ == true) {                                      \
      constexpr auto use_cache_t = true;                            \
      return __VA_ARGS__();                                                    \
    }                                                                          \
    return;                                                                    \
  }()


////////////////////////////////////////////////////////////////////////////////
// Kernel Definitions
////////////////////////////////////////////////////////////////////////////////
Tensor
split_embedding_nobag_codegen_forward_unweighted_cuda(
    const Tensor& dev_weights,
    const Tensor& uvm_weights,
    const Tensor& lxu_cache_weights,
    const Tensor& weights_placements,
    const Tensor& weights_offsets,
    const c10::SymInt D_,
    const Tensor& indices,
    const Tensor& offsets,
    const Tensor& lxu_cache_locations,
    const Tensor& uvm_cache_stats,
    const int64_t output_dtype,
    const bool is_experimental
) {
    const int64_t D = D_.guard_int(__FILE__, __LINE__);

    TENSORS_ON_SAME_CUDA_GPU_IF_NOT_OPTIONAL(
        uvm_weights,
        lxu_cache_weights,
        weights_placements,
        weights_offsets,
        indices,
        offsets,
        lxu_cache_locations,
        dev_weights
    );

    CUDA_DEVICE_GUARD(dev_weights);
    int32_t total_L = indices.numel();
    int32_t T = weights_offsets.numel();
    TORCH_CHECK_GT(T, 0);
    // offsets = [B x T  + 1]
    const auto total_B = offsets.size(0) - 1;
    const int32_t B = total_B / T;
    TORCH_CHECK_GE(B, 0);
    TORCH_CHECK_GT(D, 0);
    TORCH_CHECK_EQ(D % 4, 0);

    Tensor output;
    SparseType o_dtype = static_cast<SparseType>(output_dtype);
    TORCH_CHECK(o_dtype == SparseType::FP32 || o_dtype == SparseType::FP16 ||
                o_dtype == SparseType::BF16 || o_dtype == SparseType::INT8);

    int64_t adjusted_D = D;
    if (o_dtype == SparseType::INT8) {
        adjusted_D += T * kINT8QparamsBytes;
    }

    output = at::empty({total_L, adjusted_D}, dev_weights.options().dtype(getScalarType(o_dtype))); // if nobag

    if (B == 0) {
        return output;
    }


  return output;
}


////////////////////////////////////////////////////////////////////////////////
// Op registrations
////////////////////////////////////////////////////////////////////////////////
TORCH_LIBRARY_FRAGMENT(fbgemm, m) {
    m.def("split_embedding_nobag_codegen_forward_unweighted_cuda("
          "    Tensor dev_weights, "
          "    Tensor uvm_weights, "
          "    Tensor lxu_cache_weights, "
          "    Tensor weights_placements, "
          "    Tensor weights_offsets, "
          "    SymInt D, "
          "    Tensor indices, "
          "    Tensor offsets, "
          "    Tensor lxu_cache_locations, "
          "    Tensor uvm_cache_stats, "
          "    int output_dtype, "
          "    bool is_experimental"
          ") -> Tensor"
    );
    DISPATCH_TO_CUDA(
        "split_embedding_nobag_codegen_forward_unweighted_cuda",
        split_embedding_nobag_codegen_forward_unweighted_cuda
    );
}
Tensor
split_embedding_codegen_forward_unweighted_cuda(
    const Tensor& dev_weights,
    const Tensor& uvm_weights,
    const Tensor& lxu_cache_weights,
    const Tensor& weights_placements,
    const Tensor& weights_offsets,
    const Tensor& D_offsets,
    const c10::SymInt total_D_,
    const c10::SymInt max_D_,
    
    const Tensor& indices,
    const Tensor& offsets,
    const int64_t pooling_mode,
    const Tensor& lxu_cache_locations,
    const Tensor& uvm_cache_stats,
    const int64_t output_dtype,
    const bool is_experimental
) {
    std::cout << "dev_weights: " << dev_weights << std::endl;
    std::cout << "uvm_weights: " << uvm_weights << std::endl;
    std::cout << "lxu_cache_weights: " << lxu_cache_weights << std::endl;
    std::cout << "weights_placements: " << weights_placements << std::endl;
    std::cout << "weights_offsets: " << weights_offsets << std::endl;
    std::cout << "D_offsets: " << D_offsets << std::endl;
    std::cout << "total_D: " << total_D_ << std::endl;
    std::cout << "max_D: " << max_D_ << std::endl;
    std::cout << "indices: " << indices << std::endl;
    std::cout << "offsets: " << offsets << std::endl;
    std::cout << "pooling_mode: " << pooling_mode << std::endl;
    std::cout << "lxu_cache_locations: " << lxu_cache_locations << std::endl;
    std::cout << "uvm_cache_stats: " << uvm_cache_stats << std::endl;
    std::cout << "output_dtype: " << output_dtype << std::endl;
    std::cout << "is_experimental: " << is_experimental << std::endl;
    // std::vector<torch::Tensor> tensors_to_save = {dev_weights, uvm_weights, lxu_cache_weights, weights_placements, weights_offsets, D_offsets, indices, offsets, lxu_cache_locations, uvm_cache_stats};
    // torch::save(tensors_to_save, "inputs.pt");

    const int64_t total_D = total_D_.guard_int(__FILE__, __LINE__);
    const int64_t max_D = max_D_.guard_int(__FILE__, __LINE__);

    TENSORS_ON_SAME_CUDA_GPU_IF_NOT_OPTIONAL(
        uvm_weights,
        lxu_cache_weights,
        weights_placements,
        weights_offsets,
        D_offsets,
        indices,
        offsets,
        lxu_cache_locations,
        dev_weights
    );

    CUDA_DEVICE_GUARD(dev_weights);
    int32_t T = D_offsets.numel() - 1;
    TORCH_CHECK_GT(T, 0);
    // offsets = [B x T  + 1]
    const auto total_B = offsets.size(0) - 1;
    const int32_t B = total_B / T;
    TORCH_CHECK_GE(B, 0);
    TORCH_CHECK_GT(total_D, 0);
    TORCH_CHECK_EQ(total_D % 4, 0);
    TORCH_CHECK_LE(max_D, 2048);

    Tensor output;
    SparseType o_dtype = static_cast<SparseType>(output_dtype);
    TORCH_CHECK(o_dtype == SparseType::FP32 || o_dtype == SparseType::FP16 ||
                o_dtype == SparseType::BF16 || o_dtype == SparseType::INT8);
    int64_t total_adjusted_D = total_D;
    if (o_dtype == SparseType::INT8) {
        total_adjusted_D += T * kINT8QparamsBytes;
    }
    output = at::empty(
        {B, total_adjusted_D},
        dev_weights.options().dtype(getScalarType(o_dtype))
    ); // if nobag

    if (B == 0) {
        return output;
    }

    DISPATCH_EMB_CACHE_OUTPUT_TYPES(
        dev_weights.scalar_type(),
        lxu_cache_weights.scalar_type(),
        output.scalar_type(),
        "batched_embedding_forward_kernel_2", [&] {
        // Check if LXU cache is used
        bool use_lxu_cache = lxu_cache_weights.numel() > 0;
        const bool is_experimental_ = (
            is_experimental && !(std::is_same<emb_t, uint8_t>() || std::is_same<output_t, uint8_t>())
        );
        // if max_D > 1024, use TBE v2
        

// #ifdef USE_ROCM
//             TORCH_CHECK(false, "is_experimental=True is not supported in ROCm");
// #else
            // Allocate num warps per table based on max_D
            const int num_warps_per_table = B * div_round_up(max_D, kWarpSize * 4);
            std::cout << "num_warps_per_table: " << num_warps_per_table << std::endl;
            std::cout << "kWarpSize: " << kWarpSize << std::endl;

            const uint32_t num_warps_per_threadblock = kForwardMaxThreads / kWarpSize;
            std::cout << "num_warps_per_threadblock: " << num_warps_per_threadblock << std::endl;


            const auto kernel_func =
              (use_lxu_cache ? split_embedding_codegen_forward_unweighted_v2_kernel<
                                  emb_t, cache_t, output_t, int64_t, true>
                              : split_embedding_codegen_forward_unweighted_v2_kernel<
                                  emb_t, cache_t, output_t, int64_t, false>);

           hipLaunchKernelGGL(( kernel_func)
              , 
                dim3(div_round_up(T * num_warps_per_table, num_warps_per_threadblock)),
                dim3(dim3(kWarpSize, num_warps_per_threadblock)),
                0,
                at::hip::getCurrentHIPStreamMasqueradingAsCUDA()
              , 
                dev_weights.data_ptr<emb_t>(),
                uvm_weights.data_ptr<emb_t>(),
                lxu_cache_weights.data_ptr<cache_t>(),
                weights_placements.data_ptr<int32_t>(),
                B,
                T,
                static_cast<PoolingMode>(pooling_mode) == PoolingMode::MEAN,
                use_lxu_cache ? lxu_cache_weights.size(1) : 0,
                FixedDivisor(num_warps_per_table),
                indices.data_ptr<int64_t>(),
                offsets.data_ptr<int64_t>(),
                reinterpret_cast<uint32_t*>(D_offsets.data_ptr<int32_t>()),
                weights_offsets.data_ptr<int64_t>(),
                lxu_cache_locations.data_ptr<int32_t>(),
                output.data_ptr<output_t>()
              );
            C10_HIP_KERNEL_LAUNCH_CHECK();
// #endif
         // if has_experimental
        });

  return output;
}


////////////////////////////////////////////////////////////////////////////////
// Op registrations
////////////////////////////////////////////////////////////////////////////////
TORCH_LIBRARY_FRAGMENT(fbgemm, m) {
    m.def("split_embedding_codegen_forward_unweighted_cuda("
          "    Tensor dev_weights, "
          "    Tensor uvm_weights, "
          "    Tensor lxu_cache_weights, "
          "    Tensor weights_placements, "
          "    Tensor weights_offsets, "
          "    Tensor D_offsets, "
          "    SymInt total_D, "
          "    SymInt max_D, "
          "    Tensor indices, "
          "    Tensor offsets, "
          "    int pooling_mode, "
          "    Tensor lxu_cache_locations, "
          "    Tensor uvm_cache_stats, "
          "    int output_dtype, "
          "    bool is_experimental"
          ") -> Tensor"
          // only split_embedding_codegen_forward_[un]weighted_cuda
          // are tested to be PT2 compliant
          , {PT2_COMPLIANT_TAG}
    );
    DISPATCH_TO_CUDA(
        "split_embedding_codegen_forward_unweighted_cuda",
        split_embedding_codegen_forward_unweighted_cuda
    );
}
    // clang-format on

int main() {
    std::vector<torch::Tensor> tensor_vec;
    torch::load(tensor_vec, "inputs.pt");
    auto dev_weights = tensor_vec[0];
    auto uvm_weights = tensor_vec[1];
    auto lxu_cache_weights = tensor_vec[2];
    auto weights_placements = tensor_vec[3];
    auto weights_offsets = tensor_vec[4];
    auto D_offsets = tensor_vec[5];
    auto indices = tensor_vec[6];
    auto offsets = tensor_vec[7];
    auto lxu_cache_locations = tensor_vec[8];
    auto uvm_cache_stats = tensor_vec[9];
    std::cout << "dev_weights: " << dev_weights << std::endl;
    std::cout << "uvm_weights: " << uvm_weights << std::endl;
    std::cout << "lxu_cache_weights: " << lxu_cache_weights << std::endl;
    std::cout << "weights_placements: " << weights_placements << std::endl;
    std::cout << "weights_offsets: " << weights_offsets << std::endl;
    std::cout << "D_offsets: " << D_offsets << std::endl;
    std::cout << "indices: " << indices << std::endl;
    std::cout << "offsets: " << offsets << std::endl;
    std::cout << "lxu_cache_locations: " << lxu_cache_locations << std::endl;
    std::cout << "uvm_cache_stats: " << uvm_cache_stats << std::endl;
    
    auto result = split_embedding_codegen_forward_unweighted_cuda(
        dev_weights,
        uvm_weights,
        lxu_cache_weights,
        weights_placements,
        weights_offsets,
        D_offsets,
        8,
        4,
        indices,
        offsets,
        0,
        lxu_cache_locations,
        uvm_cache_stats,
        0,
        1
    );
    std::cout << "Result: " << result << std::endl;
    
  return 0;
}
